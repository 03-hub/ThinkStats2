{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemEvalNLPTask-LSTM+BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+hFwlWbrae+lYFrZzPFl6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/03-hub/ThinkStats2/blob/master/SemEvalNLPTask_LSTM%2BBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rekngpjx-TUZ",
        "outputId": "07b0e9cd-43d5-4342-a603-be1b3c6705f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path '/content/train' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "!git clone https://github.com/ncg-task/training-data.git \"/content/train\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FxZBEHiWcYDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import random\n",
        "!pip install transformers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertConfig, BertTokenizerFast, TFBertForSequenceClassification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqcsRAXJN9ws",
        "outputId": "a69c50c2-0853-49bc-e851-80aeb4c71b6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qx5Coh_l-64T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Articlesandcontributions(path):\n",
        "    articles = []\n",
        "    contributions = []\n",
        "\n",
        "    for task_name in os.listdir(path):  #/training-data\n",
        "        if task_name != 'README.md' and task_name != '.git':\n",
        "          article_category = os.path.join(path, task_name)  # ./training-data/natural_language_inference\n",
        "\n",
        "          for folder_name in sorted(os.listdir(article_category)):\n",
        "              article_index = os.path.join(article_category, folder_name)  # ./datasets/training-data/natural_language_inference/0\n",
        "\n",
        "              with open(glob.glob(os.path.join(article_index, '*-Stanza-out.txt'))[0], encoding='utf-8') as f:#I used glob to retrieve files with matching pattern\n",
        "                  article = f.read()                                                                           #* wild card matches the files with stanze out.txt\n",
        "                  articles.append(article.lower())\n",
        "                  \n",
        "              with open(os.path.join(article_index, 'sentences.txt'), encoding='utf-8') as f:\n",
        "                  contribution = []\n",
        "                  for line in f.readlines():\n",
        "                      article_contribution = int(line.strip())\n",
        "                      contribution.append(article_contribution)\n",
        "                  contributions.append(contribution) #contribution=[99,10.....]\n",
        "          #     break\n",
        "          # break\n",
        "    return articles, contributions"
      ],
      "metadata": {
        "id": "Rte3PxsDOvSJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SentenceAndLables(articles, contributions):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    for i, article in enumerate(articles):\n",
        "        contribution = contributions[i]\n",
        "\n",
        "        sents = article.split('\\n')[0:-1]\n",
        "        rand_sents=random.sample(sents,60)\n",
        "        #random sampling.Choosing 60 random sentences in an article because the input is huge and this device is \n",
        "        #not supporting\n",
        "        for j, sent in enumerate(rand_sents):\n",
        "            sentences.append(sent)\n",
        "            if (j + 1) in contribution:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "jKDQQqdIO4aO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ShUdJw2o_Ksk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_dir='/content/train'"
      ],
      "metadata": {
        "id": "yi_WrwLRO8Mu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_articles, train_contributions = Articlesandcontributions(train_input_dir)\n",
        "\n",
        "train_sentences, train_labels = SentenceAndLables(train_articles, train_contributions)\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_sentences, train_labels, test_size=.2)\n",
        "#def train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
        "\n",
        "#Split arrays or matrices into random train and test subsets.\n",
        "\n",
        "#list: train_sentences\n",
        "#(11376 items) ['abstract', 'for instance in the ...', 'sqs is a..]\n",
        "#list: train_labels\n",
        "#(11376 items) [0, 1, 0, 0, 0, ...]\n",
        "\n",
        "#list: val_sentences\n",
        "#(2844 items) ['the importance score...', 'this model encodes a...]\n",
        "#list: val_labels\n",
        "#(2844 items) [1, 0, 1, 0, 1, ...]\n",
        "print(len(train_labels))\n",
        "print(len(val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XE7wS3pPAFJ",
        "outputId": "86da8121-fac2-45b7-fadb-10b12dd19818"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11376\n",
            "2844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########### Library import and random seed\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "seed_val = 66\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "#### Hyper-parameters for the model\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-03\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\t\t\t\n",
        "MAX_LEN = 100\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n"
      ],
      "metadata": {
        "id": "r8Ro5gP9PKbJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#### Create Dataset Loader\n",
        "class Triage():\n",
        "    def __init__(self, data_in,data_out, tokenizer, max_len):\n",
        "        self.len = len(data_in)\n",
        "        self.data = data_in\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label = data_out\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # title = str(self.data.TITLE[index])\n",
        "        # title = \" \".join(title.split())\n",
        "        title = self.data[index]\n",
        "        inputs = self.tokenizer.encode_plus(title,\n",
        "            None,add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_length = True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        lengths = inputs['length']\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'lengths': torch.tensor(lengths, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.label[index], dtype=torch.long)\n",
        "        } \n",
        "    def __len__(self):\n",
        "      return self.len\n",
        "  "
      ],
      "metadata": {
        "id": "2hcvGOc5Pqh6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#### Train and Valid Loader \n",
        "from torch.utils.data import DataLoader\n",
        "training_set = Triage(train_sentences,train_labels, tokenizer, MAX_LEN)\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,'shuffle': True,'num_workers': 0}\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "valid_set = Triage(val_sentences,val_labels, tokenizer, MAX_LEN)\n",
        "valid_params = {'batch_size': VALID_BATCH_SIZE,'shuffle': True,'num_workers': 0}\n",
        "valid_loader = DataLoader(valid_set, **valid_params)"
      ],
      "metadata": {
        "id": "_d6RomhPQvCr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from torch import cuda\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "class SCIBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SCIBERTClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased',  output_hidden_states=True)\n",
        "        self.lstm = torch.nn.LSTM(768, 400, num_layers=2, batch_first = True, bidirectional=True)\n",
        "        self.l2 = torch.nn.Dropout(0.1)\n",
        "        self.l3 = torch.nn.Linear( 800,400)\n",
        "        self.l4=torch.nn.Linear(400,100)\n",
        "        self.l5=torch.nn.Linear(100,2)\n",
        "\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids, lengths):\n",
        "        encoded_layers = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)[2]\n",
        "        scibert_hidden_layer = encoded_layers[12]\n",
        "        enc_hiddens, (last_hidden, last_cell) = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(scibert_hidden_layer, lengths, batch_first=True, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes\n",
        "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
        "        output_hidden = self.l2(output_hidden)            # no size change\n",
        "        output_2 = self.l3(output_hidden)\n",
        "        output_2 = torch.nn.ReLU()(output_2)\n",
        "        output_4 = self.l4(output_2)\n",
        "        output_5 = self.l5(output_4)\n",
        "        return output_5\n",
        "\n",
        "model = SCIBERTClass()\n",
        "model.to(device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "luARtPFjtiu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dbd713a-4dcb-449f-8902-aa59dc404452"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SCIBERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (lstm): LSTM(768, 400, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (l2): Dropout(p=0.1, inplace=False)\n",
              "  (l3): Linear(in_features=800, out_features=400, bias=True)\n",
              "  (l4): Linear(in_features=400, out_features=100, bias=True)\n",
              "  (l5): Linear(in_features=100, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Optimizer, Loss function and evaluation metric\n",
        "optimizer = torch.optim.AdamW(params = model.parameters(), lr=LEARNING_RATE)\n",
        "c_weights = torch.tensor([list(Counter(train_labels).values())[0], list(Counter(train_labels).values())[1]], dtype=torch.float32).to(device)\n",
        "#Counter.values() helps to see the frequencies of each unique element\n",
        "c_weights = c_weights/c_weights.sum()\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight = c_weights)\n",
        "print(\"class weights\",c_weights)\n",
        "def calcuate_accu(big_idx, targets):\n",
        "  n_correct = torch.sum((big_idx==targets) * (targets==1)).item()\n",
        "  return n_correct\n",
        "def calculate_labels(targets):\n",
        "  return torch.sum(targets==1).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARkocPwv8RQr",
        "outputId": "59ee1a65-9a85-4c2c-e998-e9ba7090da95"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class weights tensor([0.8870, 0.1130], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Train one epoch\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    n_predict = 0\n",
        "    n_ground = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        lengths = data['lengths'].squeeze(1)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "        outputs = model(ids, mask,token_type_ids, lengths)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "        n_predict +=calculate_labels(big_idx)\n",
        "        n_ground +=calculate_labels(targets)\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        if _%1000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 1000 steps: {loss_step}\")\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Accuracy Epoch {epoch}: {epoch_accu} and Loss: {epoch_loss}\")\n",
        "    P = n_correct/n_predict if n_predict else 0.0\n",
        "    R = n_correct/n_ground if n_ground else 0.0\n",
        "    F1 = (2*P*R)/(P+R) if P+R else 0.0\n",
        "    print(f\"Training P and R and F1 Epoch {epoch}: {P},{R}, {F1}\")\n",
        "    return\n",
        "\n",
        "#### Valid one epoch\n",
        "def valid(epoch,model, valid_loader):\n",
        "    model.eval()\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    n_correct = 0\n",
        "    n_predict = 0\n",
        "    n_ground = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(valid_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            lengths = data['lengths'].squeeze(1)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask,token_type_ids, lengths)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "            n_predict +=calculate_labels(big_idx)\n",
        "            n_ground +=calculate_labels(targets)\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Accuracy Epoch {epoch}: {epoch_accu} and Loss: {epoch_loss}\")\n",
        "    P = n_correct/n_predict if n_predict else 0.0\n",
        "    R = n_correct/n_ground if n_ground else 0.0\n",
        "    F1 = (2*P*R)/(P+R) if P+R else 0.0\n",
        "    print(f\"Valiadation  P and R and F1 Epoch {epoch}: {P},{R}, {F1}\")\n",
        "    return epoch_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "oOfgs4czt0Ww"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model.\n",
        "for epoch in range(3):\n",
        "    train(epoch)\n",
        "    epoch_loss = valid(epoch,model, valid_loader)\n",
        "    torch.save(model,f\"/content/drive/MyDrive/task-1.pt\")"
      ],
      "metadata": {
        "id": "Z6Vgnztct-v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e11bd0-cf63-4812-d274-3f1ad9dcb190"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 1000 steps: 0.015750136226415634\n",
            "Training Loss per 1000 steps: 0.09172578943013163\n",
            "Training Accuracy Epoch 0: 0.0 and Loss: 0.0929053740103699\n",
            "Training P and R and F1 Epoch 0: 0.0,0.0, 0.0\n",
            "Validation Accuracy Epoch 0: 0.0 and Loss: 0.09361044676932559\n",
            "Valiadation  P and R and F1 Epoch 0: 0.0,0.0, 0.0\n",
            "Training Loss per 1000 steps: 0.17975355684757233\n",
            "Training Loss per 1000 steps: 0.09304322965059812\n",
            "Training Accuracy Epoch 1: 0.0 and Loss: 0.0935872724890819\n",
            "Training P and R and F1 Epoch 1: 0.0,0.0, 0.0\n",
            "Validation Accuracy Epoch 1: 0.0 and Loss: 0.09437640069387435\n",
            "Valiadation  P and R and F1 Epoch 1: 0.0,0.0, 0.0\n",
            "Training Loss per 1000 steps: 0.0134670315310359\n",
            "Training Loss per 1000 steps: 0.09227980047066163\n",
            "Training Accuracy Epoch 2: 0.0 and Loss: 0.09176475475391611\n",
            "Training P and R and F1 Epoch 2: 0.0,0.0, 0.0\n",
            "Validation Accuracy Epoch 2: 0.0 and Loss: 0.09777844749558508\n",
            "Valiadation  P and R and F1 Epoch 2: 0.0,0.0, 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ncg-task/test-data.git \"/content/test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KadeoWXJSb-",
        "outputId": "4b9e4179-9917-48af-f0ff-32d1799b3ca6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/test'...\n",
            "remote: Enumerating objects: 2508, done.\u001b[K\n",
            "remote: Total 2508 (delta 0), reused 0 (delta 0), pack-reused 2508\u001b[K\n",
            "Receiving objects: 100% (2508/2508), 215.28 MiB | 14.99 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "Checking out files: 100% (2060/2060), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Articlesandcontributions2(data_dir):\n",
        "    articles = []\n",
        "    contributions = []\n",
        "\n",
        "    for category in os.listdir(data_dir):\n",
        "        if category != 'README.md' and category != '.git' and category != 'submission.zip':\n",
        "          article_category = os.path.join(data_dir, category)\n",
        "          # print(article_category)\n",
        "\n",
        "          for foldname in sorted(os.listdir(article_category)):\n",
        "              article_index = os.path.join(article_category, foldname)\n",
        "\n",
        "              # print(glob.glob(os.path.join(article_index, '*-Stanza-out.txt')))\n",
        "              # if len(glob.glob(os.path.join(article_index, '*-Stanza-out.txt'))) != 0:\n",
        "              with open(glob.glob(os.path.join(article_index, '*-Stanza-out.txt'))[0], encoding='utf-8') as f:\n",
        "                  article = f.read()\n",
        "                  articles.append(article.lower())\n",
        "\n",
        "              with open(os.path.join(article_index, 'sentences.txt'), encoding='utf-8') as f:\n",
        "                  contribution = []\n",
        "                  for line in f.readlines():\n",
        "                      article_contribution = int(line.strip())\n",
        "                      contribution.append(article_contribution)\n",
        "                  contributions.append(contribution)\n",
        "              # break\n",
        "          # break\n",
        "    return articles, contributions\n",
        "\n",
        "def SentenceAndLables2(articles, contributions):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    for i, article in enumerate(articles):\n",
        "        contribution = contributions[i]\n",
        "\n",
        "        sents = article.split('\\n')[0:-1]\n",
        "        rand_sents=random.sample(sents,30)\n",
        "        # sents = article.split('\\n')\n",
        "        rand_sents=random.sample(rand_sents,10)\n",
        "        for row, sent in enumerate(rand_sents):\n",
        "            sentences.append(sent)\n",
        "            \n",
        "            if (row + 1) in contribution:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "    # print(sentences)\n",
        "    # print(labels)\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "Xwyl9SVzIR1N"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_dir='/content/test'\n",
        "\n",
        "test_articles, test_contributions = Articlesandcontributions2(test_input_dir)\n",
        "\n",
        "test_sentences, test_labels = SentenceAndLables2(test_articles, test_contributions)\n",
        "test_set = Triage(test_sentences,test_labels, tokenizer, MAX_LEN)\n",
        "test_params = {'batch_size': 8,'shuffle': True,'num_workers': 0}\n",
        "testing_loader = DataLoader(test_set, **test_params)\n"
      ],
      "metadata": {
        "id": "YJlHns0jJfnb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _,data in enumerate(testing_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        lengths = data['lengths'].squeeze(1)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "        outputs = model(ids, mask,token_type_ids, lengths)\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "aNk1im2aFc83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}